{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3548c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b736d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('illinois_basing_train.csv')\n",
    "\n",
    "\n",
    "df_train[df_train.columns[-1]].fillna(0, inplace = True)\n",
    "y = df_train[df_train.columns[-1]]\n",
    "df_train.drop(df_train.columns[-1], axis=1, inplace = True)\n",
    "\n",
    "df_train['Month'] = pd.to_datetime(df_train['SampleTimeUTC']).dt.month\n",
    "df_train['Day'] = pd.to_datetime(df_train['SampleTimeUTC']).dt.day\n",
    "df_train['Hour'] = pd.to_datetime(df_train['SampleTimeUTC']).dt.hour\n",
    "df_train['Year'] = pd.DatetimeIndex(df_train['SampleTimeUTC']).year\n",
    "df_train.drop('SampleTimeUTC', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db42420",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [i for i in df_train.columns if df_train[i].isnull().any()]\n",
    "for i in cols:\n",
    "    df_train[i].fillna(df_train[i].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645c23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping (see Module 3.4)\n",
    "class EarlyStopping():\n",
    "  def __init__(self, patience=10, min_delta=1e-4, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss < self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d29420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 0.8088574409484863, vloss: 15.331805, EStop:[0/10]: 100%|████████| 1542/1542 [00:06<00:00, 230.79it/s]\n",
      "Epoch: 2, tloss: 0.025616204366087914, vloss: 14.915764, EStop:[0/10]: 100%|██████| 1542/1542 [00:05<00:00, 294.15it/s]\n",
      "Epoch: 3, tloss: 0.06818347424268723, vloss: 16.074657, EStop:[1/10]: 100%|███████| 1542/1542 [00:06<00:00, 233.93it/s]\n",
      "Epoch: 4, tloss: 0.06609426438808441, vloss: 14.749968, EStop:[0/10]: 100%|███████| 1542/1542 [00:06<00:00, 236.16it/s]\n",
      "Epoch: 5, tloss: 0.029554683715105057, vloss: 14.756166, EStop:[1/10]: 100%|██████| 1542/1542 [00:05<00:00, 263.93it/s]\n",
      "Epoch: 6, tloss: 0.04469485208392143, vloss: 14.757010, EStop:[2/10]: 100%|███████| 1542/1542 [00:05<00:00, 283.33it/s]\n",
      "Epoch: 7, tloss: 0.06110917404294014, vloss: 14.771641, EStop:[3/10]: 100%|███████| 1542/1542 [00:05<00:00, 282.32it/s]\n",
      "Epoch: 8, tloss: 0.09462262690067291, vloss: 14.775132, EStop:[4/10]: 100%|███████| 1542/1542 [00:06<00:00, 228.84it/s]\n",
      "Epoch: 9, tloss: 0.07244472950696945, vloss: 14.772689, EStop:[5/10]: 100%|███████| 1542/1542 [00:06<00:00, 252.93it/s]\n",
      "Epoch: 10, tloss: 0.10247889161109924, vloss: 14.782522, EStop:[6/10]: 100%|██████| 1542/1542 [00:06<00:00, 232.98it/s]\n",
      "Epoch: 11, tloss: 0.07292822003364563, vloss: 14.784867, EStop:[7/10]: 100%|██████| 1542/1542 [00:06<00:00, 250.36it/s]\n",
      "Epoch: 12, tloss: 0.13700467348098755, vloss: 14.791247, EStop:[8/10]: 100%|██████| 1542/1542 [00:06<00:00, 242.41it/s]\n",
      "Epoch: 13, tloss: 0.06425432860851288, vloss: 14.789720, EStop:[9/10]: 100%|██████| 1542/1542 [00:06<00:00, 245.03it/s]\n",
      "Epoch: 14, tloss: 0.04023704677820206, vloss: 14.797400, EStop:[Stopped on 10]: 100%|█| 1542/1542 [00:06<00:00, 238.35i\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available. (see Module 3.2)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, out_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "x = df_train  \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x.values, y.values, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Numpy to Torch Tensor\n",
    "x_train = torch.Tensor(x_train).float()\n",
    "y_train = torch.Tensor(y_train).float()\n",
    "\n",
    "x_test = torch.Tensor(x_test).float().to(device)\n",
    "y_test = torch.Tensor(y_test).float().to(device)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Net(x.shape[1],1).to(device)\n",
    "\n",
    "# Define the loss function for regression\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch<1000 and not done:\n",
    "  epoch += 1\n",
    "  steps = list(enumerate(dataloader_train))\n",
    "  pbar = tqdm.tqdm(steps)\n",
    "  model.train()\n",
    "  for i, (x_batch, y_batch) in pbar:\n",
    "    y_batch_pred = model(x_batch.to(device)).flatten()\n",
    "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "    if i == len(steps)-1:\n",
    "      model.eval()\n",
    "      pred = model(x_test).flatten()\n",
    "      vloss = loss_fn(pred, y_test)\n",
    "      if es(model,vloss): done = True\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "    else:\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d81362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('illinois_basing_test.csv')\n",
    "df_test['Month'] = pd.to_datetime(df_test['SampleTimeUTC']).dt.month\n",
    "df_test['Day'] = pd.to_datetime(df_test['SampleTimeUTC']).dt.day\n",
    "df_test['Hour'] = pd.to_datetime(df_test['SampleTimeUTC']).dt.hour\n",
    "df_test['Year'] = pd.DatetimeIndex(df_test['SampleTimeUTC']).year\n",
    "df_test.drop('SampleTimeUTC', axis=1, inplace = True)\n",
    "cols = [i for i in df_test.columns if df_test[i].isnull().any()]\n",
    "for i in cols:\n",
    "    df_test[i].fillna(df_test[i].mean(), inplace=True)\n",
    "x_test = df_test.values\n",
    "x_test = torch.Tensor(x_test).float().to(device)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict\n",
    "pred = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb1406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.cpu().detach().numpy()\n",
    "preds = pd.DataFrame(pred, columns=['inj_diff\\xa0'])\n",
    "preds.to_csv(\"basic_nn.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c343037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 1.0855745077133179, vloss: 21.721550, EStop:[0/50]: 100%|████████| 1542/1542 [00:05<00:00, 297.60it/s]\n",
      "Epoch: 2, tloss: 146.0690155029297, vloss: 37.612030, EStop:[1/50]: 100%|█████████| 1542/1542 [00:05<00:00, 298.10it/s]\n",
      "Epoch: 3, tloss: 2.2170002460479736, vloss: 17.593355, EStop:[0/50]: 100%|████████| 1542/1542 [00:05<00:00, 296.89it/s]\n",
      "Epoch: 4, tloss: 12.683396339416504, vloss: 19.668217, EStop:[1/50]: 100%|████████| 1542/1542 [00:05<00:00, 281.44it/s]\n",
      "Epoch: 5, tloss: 0.3144833445549011, vloss: 19.195494, EStop:[2/50]: 100%|████████| 1542/1542 [00:05<00:00, 285.05it/s]\n",
      "Epoch: 6, tloss: 0.007501082960516214, vloss: 15.801548, EStop:[0/50]: 100%|██████| 1542/1542 [00:06<00:00, 252.57it/s]\n",
      "Epoch: 7, tloss: 0.018034078180789948, vloss: 14.869804, EStop:[0/50]: 100%|██████| 1542/1542 [00:05<00:00, 284.05it/s]\n",
      "Epoch: 8, tloss: 1.4217360019683838, vloss: 16.011496, EStop:[1/50]: 100%|████████| 1542/1542 [00:05<00:00, 279.65it/s]\n",
      "Epoch: 9, tloss: 0.030279282480478287, vloss: 14.781831, EStop:[0/50]: 100%|██████| 1542/1542 [00:05<00:00, 291.57it/s]\n",
      "Epoch: 10, tloss: 0.008709048852324486, vloss: 14.728909, EStop:[0/50]: 100%|█████| 1542/1542 [00:05<00:00, 273.36it/s]\n",
      "Epoch: 11, tloss: 0.003951552323997021, vloss: 14.711527, EStop:[0/50]: 100%|█████| 1542/1542 [00:05<00:00, 263.85it/s]\n",
      "Epoch: 12, tloss: 0.0036104328464716673, vloss: 14.717052, EStop:[1/50]: 100%|████| 1542/1542 [00:05<00:00, 277.46it/s]\n",
      "Epoch: 13, tloss: 0.02647966332733631, vloss: 14.709121, EStop:[2/50]: 100%|██████| 1542/1542 [00:05<00:00, 262.75it/s]\n",
      "Epoch: 14, tloss: 0.017628241330385208, vloss: 14.709913, EStop:[3/50]: 100%|█████| 1542/1542 [00:07<00:00, 217.07it/s]\n",
      "Epoch: 15, tloss: 0.17485477030277252, vloss: 14.714252, EStop:[4/50]: 100%|██████| 1542/1542 [00:06<00:00, 246.90it/s]\n",
      "Epoch: 16, tloss: 0.014071790501475334, vloss: 14.715622, EStop:[5/50]: 100%|█████| 1542/1542 [00:05<00:00, 297.68it/s]\n",
      "Epoch: 17, tloss: 0.049592144787311554, vloss: 14.725247, EStop:[6/50]: 100%|█████| 1542/1542 [00:04<00:00, 309.33it/s]\n",
      "Epoch: 18, tloss: 0.011459841392934322, vloss: 14.724716, EStop:[7/50]: 100%|█████| 1542/1542 [00:05<00:00, 294.59it/s]\n",
      "Epoch: 19, tloss: 0.004048897884786129, vloss: 14.736159, EStop:[8/50]: 100%|█████| 1542/1542 [00:05<00:00, 270.72it/s]\n",
      "Epoch: 20, tloss: 0.011886690743267536, vloss: 14.741503, EStop:[9/50]: 100%|█████| 1542/1542 [00:05<00:00, 275.59it/s]\n",
      "Epoch: 21, tloss: 0.022956691682338715, vloss: 14.745681, EStop:[10/50]: 100%|████| 1542/1542 [00:05<00:00, 301.19it/s]\n",
      "Epoch: 22, tloss: 0.0098728621378541, vloss: 14.745115, EStop:[11/50]: 100%|██████| 1542/1542 [00:05<00:00, 301.39it/s]\n",
      "Epoch: 23, tloss: 0.014010138809680939, vloss: 14.750246, EStop:[12/50]: 100%|████| 1542/1542 [00:05<00:00, 305.48it/s]\n",
      "Epoch: 24, tloss: 0.24171802401542664, vloss: 14.759703, EStop:[13/50]: 100%|█████| 1542/1542 [00:05<00:00, 304.61it/s]\n",
      "Epoch: 25, tloss: 0.15152640640735626, vloss: 14.761601, EStop:[14/50]: 100%|█████| 1542/1542 [00:05<00:00, 302.45it/s]\n",
      "Epoch: 26, tloss: 0.30734390020370483, vloss: 14.763252, EStop:[15/50]: 100%|█████| 1542/1542 [00:05<00:00, 304.57it/s]\n",
      "Epoch: 27, tloss: 0.02185518853366375, vloss: 14.766318, EStop:[16/50]: 100%|█████| 1542/1542 [00:05<00:00, 303.86it/s]\n",
      "Epoch: 28, tloss: 0.04381059855222702, vloss: 14.766857, EStop:[17/50]: 100%|█████| 1542/1542 [00:05<00:00, 303.56it/s]\n",
      "Epoch: 29, tloss: 0.03650161251425743, vloss: 14.765834, EStop:[18/50]: 100%|█████| 1542/1542 [00:05<00:00, 300.91it/s]\n",
      "Epoch: 30, tloss: 0.045813992619514465, vloss: 14.770312, EStop:[19/50]: 100%|████| 1542/1542 [00:05<00:00, 303.16it/s]\n",
      "Epoch: 31, tloss: 0.060916922986507416, vloss: 14.770637, EStop:[20/50]: 100%|████| 1542/1542 [00:05<00:00, 295.09it/s]\n",
      "Epoch: 32, tloss: 0.06529881060123444, vloss: 14.779266, EStop:[21/50]: 100%|█████| 1542/1542 [00:05<00:00, 301.86it/s]\n",
      "Epoch: 33, tloss: 0.06293801963329315, vloss: 14.775715, EStop:[22/50]: 100%|█████| 1542/1542 [00:05<00:00, 303.91it/s]\n",
      "Epoch: 34, tloss: 0.1348467767238617, vloss: 14.788312, EStop:[23/50]: 100%|██████| 1542/1542 [00:05<00:00, 303.68it/s]\n",
      "Epoch: 35, tloss: 0.0406067818403244, vloss: 14.783265, EStop:[24/50]: 100%|██████| 1542/1542 [00:05<00:00, 305.55it/s]\n",
      "Epoch: 36, tloss: 0.041623108088970184, vloss: 14.784357, EStop:[25/50]: 100%|████| 1542/1542 [00:05<00:00, 301.70it/s]\n",
      "Epoch: 37, tloss: 0.05267522856593132, vloss: 14.783769, EStop:[26/50]: 100%|█████| 1542/1542 [00:04<00:00, 308.42it/s]\n",
      "Epoch: 38, tloss: 0.1325676292181015, vloss: 14.784605, EStop:[27/50]: 100%|██████| 1542/1542 [00:05<00:00, 302.04it/s]\n",
      "Epoch: 39, tloss: 0.0521458201110363, vloss: 14.786292, EStop:[28/50]: 100%|██████| 1542/1542 [00:05<00:00, 302.78it/s]\n",
      "Epoch: 40, tloss: 0.00021276515326462686, vloss: 14.793667, EStop:[29/50]: 100%|██| 1542/1542 [00:05<00:00, 301.84it/s]\n",
      "Epoch: 41, tloss: 0.07349605113267899, vloss: 14.793632, EStop:[30/50]: 100%|█████| 1542/1542 [00:05<00:00, 299.40it/s]\n",
      "Epoch: 42, tloss: 0.03572356700897217, vloss: 14.790902, EStop:[31/50]: 100%|█████| 1542/1542 [00:05<00:00, 289.21it/s]\n",
      "Epoch: 43, tloss: 0.20980168879032135, vloss: 14.801644, EStop:[32/50]: 100%|█████| 1542/1542 [00:05<00:00, 286.07it/s]\n",
      "Epoch: 44, tloss: 0.06909309327602386, vloss: 14.805386, EStop:[33/50]: 100%|█████| 1542/1542 [00:05<00:00, 288.53it/s]\n",
      "Epoch: 45, tloss: 0.41635388135910034, vloss: 14.806690, EStop:[34/50]: 100%|█████| 1542/1542 [00:05<00:00, 301.79it/s]\n",
      "Epoch: 46, tloss: 0.06978020071983337, vloss: 14.807233, EStop:[35/50]: 100%|█████| 1542/1542 [00:05<00:00, 300.25it/s]\n",
      "Epoch: 47, tloss: 0.1226060688495636, vloss: 14.808090, EStop:[36/50]: 100%|██████| 1542/1542 [00:05<00:00, 301.18it/s]\n",
      "Epoch: 48, tloss: 0.05102430656552315, vloss: 14.806558, EStop:[37/50]: 100%|█████| 1542/1542 [00:05<00:00, 299.95it/s]\n",
      "Epoch: 49, tloss: 0.03764674812555313, vloss: 14.799089, EStop:[38/50]: 100%|█████| 1542/1542 [00:05<00:00, 296.52it/s]\n",
      "Epoch: 50, tloss: 0.06796294450759888, vloss: 14.798302, EStop:[39/50]: 100%|█████| 1542/1542 [00:05<00:00, 295.49it/s]\n",
      "Epoch: 51, tloss: 0.054041385650634766, vloss: 14.799629, EStop:[40/50]: 100%|████| 1542/1542 [00:05<00:00, 295.04it/s]\n",
      "Epoch: 52, tloss: 0.09835585951805115, vloss: 14.796317, EStop:[41/50]: 100%|█████| 1542/1542 [00:05<00:00, 286.30it/s]\n",
      "Epoch: 53, tloss: 0.0486576072871685, vloss: 14.803902, EStop:[42/50]: 100%|██████| 1542/1542 [00:05<00:00, 283.31it/s]\n",
      "Epoch: 54, tloss: 0.014178561046719551, vloss: 14.807365, EStop:[43/50]: 100%|████| 1542/1542 [00:05<00:00, 287.74it/s]\n",
      "Epoch: 55, tloss: 2.934699296951294, vloss: 14.807261, EStop:[44/50]: 100%|███████| 1542/1542 [00:05<00:00, 292.33it/s]\n",
      "Epoch: 56, tloss: 0.0797971710562706, vloss: 14.809572, EStop:[45/50]: 100%|██████| 1542/1542 [00:05<00:00, 294.53it/s]\n",
      "Epoch: 57, tloss: 0.06619566679000854, vloss: 14.805914, EStop:[46/50]: 100%|█████| 1542/1542 [00:05<00:00, 297.25it/s]\n",
      "Epoch: 58, tloss: 0.08132055401802063, vloss: 14.815157, EStop:[47/50]: 100%|█████| 1542/1542 [00:05<00:00, 287.64it/s]\n",
      "Epoch: 59, tloss: 28.890832901000977, vloss: 14.810107, EStop:[48/50]: 100%|██████| 1542/1542 [00:05<00:00, 258.97it/s]\n",
      "Epoch: 60, tloss: 0.06835009157657623, vloss: 14.794318, EStop:[49/50]: 100%|█████| 1542/1542 [00:05<00:00, 268.55it/s]\n",
      "Epoch: 61, tloss: 0.20467139780521393, vloss: 14.812118, EStop:[Stopped on 50]: 100%|█| 1542/1542 [00:05<00:00, 270.17i\n"
     ]
    }
   ],
   "source": [
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out\n",
    "x = df_train  \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x.values, y.values, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Numpy to Torch Tensor\n",
    "x_train = torch.Tensor(x_train).float()\n",
    "y_train = torch.Tensor(y_train).float()\n",
    "\n",
    "x_test = torch.Tensor(x_test).float().to(device)\n",
    "y_test = torch.Tensor(y_test).float().to(device)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Net(x.shape[1],1).to(device)\n",
    "\n",
    "# Define the loss function for regression\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch<1000 and not done:\n",
    "  epoch += 1\n",
    "  steps = list(enumerate(dataloader_train))\n",
    "  pbar = tqdm.tqdm(steps)\n",
    "  model.train()\n",
    "  for i, (x_batch, y_batch) in pbar:\n",
    "    y_batch_pred = model(x_batch.to(device)).flatten()\n",
    "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "    if i == len(steps)-1:\n",
    "      model.eval()\n",
    "      pred = model(x_test).flatten()\n",
    "      vloss = loss_fn(pred, y_test)\n",
    "      if es(model,vloss): done = True\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "    else:\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d530b606",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7424\\725984194.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtesting_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdataset_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m dataloader_test = DataLoader(dataset_test,\\\n\u001b[0;32m     17\u001b[0m   batch_size=1, shuffle=True)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37gpu\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Size mismatch between tensors\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37gpu\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Size mismatch between tensors\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "testing_values = df_test.values\n",
    "dataset_test = TensorDataset(testing_values)\n",
    "dataloader_test = DataLoader(dataset_test,\\\n",
    "  batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "predictions = []\n",
    "model.eval()\n",
    "for i in dataloader_test:\n",
    "    with torch.no_grad():\n",
    "        pred = model(i).to(device)\n",
    "        predictions.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21125b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

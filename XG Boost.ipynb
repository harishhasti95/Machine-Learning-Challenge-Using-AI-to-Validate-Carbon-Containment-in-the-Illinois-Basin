{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b71359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39eb62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('illinois_basing_train.csv')\n",
    "df_train.drop('Avg_PLT_CO2InjRate_TPH', axis=1, inplace = True)\n",
    "df_test = pd.read_csv('illinois_basing_test.csv')\n",
    "\n",
    "df_train[df_train.columns[-1]].fillna(0, inplace = True)\n",
    "y = df_train[df_train.columns[-1]]\n",
    "df_train.drop(df_train.columns[-1], axis=1, inplace = True)\n",
    "\n",
    "df_train['Month'] = pd.to_datetime(df_train['SampleTimeUTC']).dt.month\n",
    "df_train['Day'] = pd.to_datetime(df_train['SampleTimeUTC']).dt.day\n",
    "df_train['Hour'] = pd.to_datetime(df_train['SampleTimeUTC']).dt.hour\n",
    "df_train['Year'] = pd.DatetimeIndex(df_train['SampleTimeUTC']).year\n",
    "df_train.drop('SampleTimeUTC', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "df_test['Month'] = pd.to_datetime(df_test['SampleTimeUTC']).dt.month\n",
    "df_test['Day'] = pd.to_datetime(df_test['SampleTimeUTC']).dt.day\n",
    "df_test['Hour'] = pd.to_datetime(df_test['SampleTimeUTC']).dt.hour\n",
    "df_test['Year'] = pd.DatetimeIndex(df_test['SampleTimeUTC']).year\n",
    "df_test.drop('SampleTimeUTC', axis=1, inplace = True)\n",
    "\n",
    "cols = [i for i in df_train.columns if df_train[i].isnull().any()]\n",
    "for i in cols:\n",
    "    df_train[i].fillna(df_train[i].mean(), inplace=True)\n",
    "\n",
    "cols = [i for i in df_test.columns if df_test[i].isnull().any()]\n",
    "for i in cols:\n",
    "    df_test[i].fillna(df_test[i].mean(), inplace=True)\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_train, y, test_size=0.1, random_state=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce90c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 12.5087\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9cf49b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 13.5357\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 10,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e594b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 13.0744\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a9fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 11.6790\n"
     ]
    }
   ],
   "source": [
    "# best till now - first result\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost3.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e938aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 9.6820\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 7,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcc1629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 9.6501\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 7,\n",
    "    \"min_samples_split\": 6,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost5.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2527b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 10.1321\n"
     ]
    }
   ],
   "source": [
    "# the best one now - second\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost6.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d6c9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 10.0390\n"
     ]
    }
   ],
   "source": [
    "# The best now - third\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.07,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost7.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa51eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 9.9803\n"
     ]
    }
   ],
   "source": [
    "# The best now -fourht\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.072,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost8.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c383984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 10.0644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"n_estimators\": 520,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.072,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost9.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc4eb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 10.0508\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"n_estimators\": 450,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.072,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost10.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27efd2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 10.0857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"n_estimators\": 490,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.072,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost10.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9cd869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Score -116.9790190527627\n",
      "Best Params {'learning_rate': 0.0718, 'max_depth': 5, 'n_estimators': 505, 'nthread': 4, 'objective': 'reg:squarederror'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Various hyper-parameters to tune\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "xgb1 = XGBRegressor(tree_method=\"gpu_hist\")\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [0.072, 0.0721, 0.0719, 0.0718], #so called `eta` value\n",
    "              'max_depth': [5],\n",
    "              'n_estimators': [507, 508, 509, 506, 505]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=4)\n",
    "\n",
    "xgb_grid.fit(df_train, y)\n",
    "\n",
    "\n",
    "print('Best Score', xgb_grid.best_score_)\n",
    "print('Best Params', xgb_grid.best_params_)\n",
    "\n",
    "\n",
    "params = xgb_grid.best_params_\n",
    "\n",
    "best_model = XGBRegressor(**params)\n",
    "best_model.fit(df_train, y)\n",
    "\n",
    "\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = best_model.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost12.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee2addbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 10.2640\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "params = {\n",
    "    \"n_estimators\": 505,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.0718,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost13.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b16a3bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 9.9588\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 506,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.072,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost14.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e5ae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 10.6699\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"n_estimators\": 506,\n",
    "    \"max_depth\": 3,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.072,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost15.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc9468",
   "metadata": {},
   "source": [
    "# Cross fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb7bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 132 candidates, totalling 528 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\anaconda3\\envs\\py37gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [-5.47227957e+01 -7.60396321e+01 -8.61692840e+01 -9.93161436e+01\n",
      " -5.59242860e+01 -7.73492874e+01 -8.98603793e+01 -1.07763633e+02\n",
      " -5.17658058e+01 -7.68737549e+01 -9.15831236e+01 -1.13138524e+02\n",
      " -5.82898480e+01 -7.12841072e+01 -8.50413569e+01 -8.82259310e+01\n",
      " -6.56088445e+01 -7.91331824e+01 -9.14787273e+01 -1.04443603e+02\n",
      " -7.50817555e+01 -9.46368046e+01 -1.11409329e+02 -1.35501923e+02\n",
      " -9.61183258e+01 -1.01358167e+02 -1.08640454e+02 -1.08343465e+02\n",
      " -9.78856436e+01 -1.20628666e+02 -1.19665908e+02 -1.22844784e+02\n",
      " -9.69775585e+01 -1.16377859e+02 -1.27092535e+02 -1.26307184e+02\n",
      " -2.33497051e-01 -6.31780566e-01 -1.15420669e+00 -2.64779692e+00\n",
      " -2.28430269e-01 -6.21312097e-01 -1.13916206e+00 -2.64497809e+00\n",
      " -2.24205216e-01 -6.13933994e-01 -1.12857129e+00 -2.62891650e+00\n",
      " -1.22209923e+01 -2.44221720e+01 -3.60877125e+01 -5.67907297e+01\n",
      " -1.21925330e+01 -2.46488558e+01 -3.65291751e+01 -5.79549385e+01\n",
      " -1.21147782e+01 -2.47443964e+01 -3.70275938e+01 -5.87634466e+01\n",
      " -1.41189736e+02 -1.64439860e+02 -1.71057336e+02 -1.77346736e+02\n",
      " -1.05489243e+02 -1.19229699e+02 -1.21931134e+02 -1.26296083e+02\n",
      " -1.12995738e+02 -1.26528771e+02 -1.27810027e+02 -1.33278744e+02\n",
      " -3.52831182e+03 -3.72641965e+03 -3.63744572e+03 -3.73354002e+03\n",
      " -3.04550876e+03 -2.85957779e+03 -2.79898318e+03 -2.83759498e+03\n",
      " -1.01631863e+03 -1.01043136e+03 -1.03085611e+03 -1.03706983e+03\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:13:29] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    callbacks=None, colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None,\n",
       "                                    early_stopping_rounds=None,\n",
       "                                    enable_categorical=False, eval_metric=None,\n",
       "                                    gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                    importance_type=None,\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, max_bin=None,\n",
       "                                    max_cat...\n",
       "                                    predictor=None, random_state=None,\n",
       "                                    reg_alpha=None, reg_lambda=None, ...),\n",
       "             n_jobs=5,\n",
       "             param_grid={'colsample_bytree': [0.7],\n",
       "                         'learning_rate': [0.03, 0.05, 0.07, 0.001, 0.01, 0.1,\n",
       "                                           1, 10, 100, 1000, 10000],\n",
       "                         'max_depth': [5, 6, 7], 'min_child_weight': [4],\n",
       "                         'n_estimators': [100, 200, 300, 500], 'nthread': [4],\n",
       "                         'objective': ['reg:squarederror'], 'silent': [1],\n",
       "                         'subsample': [0.7]},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Various hyper-parameters to tune\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "xgb1 = XGBRegressor(tree_method=\"gpu_hist\")\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [.03, 0.05, .07, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000], #so called `eta` value\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [100, 200, 300, 500]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 4,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(df_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5691b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score -0.22420521594915105\n",
      "Best Params {'colsample_bytree': 0.7, 'learning_rate': 0.001, 'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 100, 'nthread': 4, 'objective': 'reg:squarederror', 'silent': 1, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "print('Best Score', xgb_grid.best_score_)\n",
    "print('Best Params', xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c5a819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:26:52] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"params\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=0, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0...depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1,\n",
       "             params={'colsample_bytree': 0.7, 'learning_rate': 0.001,\n",
       "                     'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 100,\n",
       "                     'nthread': 4, 'objective': 'reg:squarederror', 'silent': 1,\n",
       "                     'subsample': 0.7},\n",
       "             predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = XGBRegressor(params = xgb_grid.best_params_, tree_method=\"gpu_hist\")\n",
    "best_model.fit(df_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa1faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb17b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost_grid.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd074e",
   "metadata": {},
   "source": [
    "# Best Model - XGBoost after regerous attemps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed63e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error (MSE) on test set: 9.8513\n"
     ]
    }
   ],
   "source": [
    "# The best now - fifth\n",
    "params = {\n",
    "    \"n_estimators\": 505,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"learning_rate\": 0.072,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_val, reg.predict(X_val))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# convert array into dataframe\n",
    "test_predict = reg.predict(df_test)\n",
    "preds = pd.DataFrame(test_predict, columns=['inj_diff'])\n",
    "\n",
    "# save the dataframe as a csv file\n",
    "preds.to_csv(\"pred_with_time_stamp_features_xg_boost11.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b3687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
